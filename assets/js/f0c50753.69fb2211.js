"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[6154],{15680:(e,t,a)=>{a.r(t),a.d(t,{MDXContext:()=>m,MDXProvider:()=>s,mdx:()=>h,useMDXComponents:()=>c,withMDXComponents:()=>p});var n=a(96540);function r(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function i(){return i=Object.assign||function(e){for(var t=1;t<arguments.length;t++){var a=arguments[t];for(var n in a)Object.prototype.hasOwnProperty.call(a,n)&&(e[n]=a[n])}return e},i.apply(this,arguments)}function o(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function l(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?o(Object(a),!0).forEach((function(t){r(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):o(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function d(e,t){if(null==e)return{};var a,n,r=function(e,t){if(null==e)return{};var a,n,r={},i=Object.keys(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||(r[a]=e[a]);return r}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(r[a]=e[a])}return r}var m=n.createContext({}),p=function(e){return function(t){var a=c(t.components);return n.createElement(e,i({},t,{components:a}))}},c=function(e){var t=n.useContext(m),a=t;return e&&(a="function"==typeof e?e(t):l(l({},t),e)),a},s=function(e){var t=c(e.components);return n.createElement(m.Provider,{value:t},e.children)},u="mdxType",x={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},y=n.forwardRef((function(e,t){var a=e.components,r=e.mdxType,i=e.originalType,o=e.parentName,m=d(e,["components","mdxType","originalType","parentName"]),p=c(a),s=r,u=p["".concat(o,".").concat(s)]||p[s]||x[s]||i;return a?n.createElement(u,l(l({ref:t},m),{},{components:a})):n.createElement(u,l({ref:t},m))}));function h(e,t){var a=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var i=a.length,o=new Array(i);o[0]=y;var l={};for(var d in t)hasOwnProperty.call(t,d)&&(l[d]=t[d]);l.originalType=e,l[u]="string"==typeof e?e:r,o[1]=l;for(var m=2;m<i;m++)o[m]=a[m];return n.createElement.apply(null,o)}return n.createElement.apply(null,a)}y.displayName="MDXCreateElement"},65064:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>d,contentTitle:()=>o,default:()=>s,frontMatter:()=>i,metadata:()=>l,toc:()=>m});var n=a(58168),r=(a(96540),a(15680));const i={sidebar_position:50,title:"Trajectory"},o="MPS output - Trajectory",l={unversionedId:"data_formats/mps/mps_trajectory",id:"data_formats/mps/mps_trajectory",title:"Trajectory",description:"Standard Machine Perception Services (MPS) outputs are:",source:"@site/docs/data_formats/mps/mps_trajectory.mdx",sourceDirName:"data_formats/mps",slug:"/data_formats/mps/mps_trajectory",permalink:"/projectaria_tools/docs/data_formats/mps/mps_trajectory",draft:!1,editUrl:"https://github.com/facebookresearch/projectaria_tools/tree/main/website/docs/data_formats/mps/mps_trajectory.mdx",tags:[],version:"current",sidebarPosition:50,frontMatter:{sidebar_position:50,title:"Trajectory"},sidebar:"tutorialSidebar",previous:{title:"Basics",permalink:"/projectaria_tools/docs/data_formats/mps/mps_summary"},next:{title:"Semi-Dense Point Cloud",permalink:"/projectaria_tools/docs/data_formats/mps/mps_pointcloud"}},d={},m=[{value:"Open loop trajectory",id:"open-loop-trajectory",level:2},{value:"Closed loop trajectory",id:"closed-loop-trajectory",level:2},{value:"Online calibration",id:"online-calibration",level:2},{value:"Static camera calibration",id:"static-camera-calibration",level:2}],p={toc:m},c="wrapper";function s(e){let{components:t,...a}=e;return(0,r.mdx)(c,(0,n.A)({},p,a,{components:t,mdxType:"MDXLayout"}),(0,r.mdx)("h1",{id:"mps-output---trajectory"},"MPS output - Trajectory"),(0,r.mdx)("p",null,"Standard ",(0,r.mdx)("a",{parentName:"p",href:"/docs/ARK/mps"},"Machine Perception Services (MPS)")," outputs are:"),(0,r.mdx)("ul",null,(0,r.mdx)("li",{parentName:"ul"},(0,r.mdx)("inlineCode",{parentName:"li"},"open_loop_trajectory.csv")," file"),(0,r.mdx)("li",{parentName:"ul"},(0,r.mdx)("inlineCode",{parentName:"li"},"closed_loop_trajectory.csv")," file"),(0,r.mdx)("li",{parentName:"ul"},(0,r.mdx)("inlineCode",{parentName:"li"},"online_calibration.jsonl")," file"),(0,r.mdx)("li",{parentName:"ul"},"Static camera calibration .csv file")),(0,r.mdx)("p",null,"When requesting MPS, Semi-Dense Point Cloud can be requested as an addition to Trajectory services."),(0,r.mdx)("h2",{id:"open-loop-trajectory"},"Open loop trajectory"),(0,r.mdx)("p",null,"Open loop trajectory is the high frequency (IMU rate, which is 1kHz) odometry estimation output by the visual-inertial odometry (VIO), in an arbitrary odometry coordinate frame. The estimation includes pose and dynamics (translational and angular velocities)."),(0,r.mdx)("p",null,"The open loop trajectory has good \u201crelative\u201d and \u201clocal\u201d accuracy: the relative transformation between two poses is accurate when the time span between two frames is short (within a few minutes). However, the open loop trajectory has increased drift error accumulated over time spent and travel distance. Consider using closed loop trajectory if you are looking for trajectory without drift error."),(0,r.mdx)("p",null,"For the utility function to load the open loop trajectory in Python and C++, please check the ",(0,r.mdx)("a",{parentName:"p",href:"/docs/data_utilities/core_code_snippets/mps#open-loopclosed-loop-trajectory"},"code examples")),(0,r.mdx)("table",null,(0,r.mdx)("thead",{parentName:"table"},(0,r.mdx)("tr",{parentName:"thead"},(0,r.mdx)("th",{parentName:"tr",align:null},"Column"),(0,r.mdx)("th",{parentName:"tr",align:null},"Type"),(0,r.mdx)("th",{parentName:"tr",align:null},"Description"))),(0,r.mdx)("tbody",{parentName:"table"},(0,r.mdx)("tr",{parentName:"tbody"},(0,r.mdx)("td",{parentName:"tr",align:null},(0,r.mdx)("inlineCode",{parentName:"td"},"tracking_timestamp_us")),(0,r.mdx)("td",{parentName:"tr",align:null},"int"),(0,r.mdx)("td",{parentName:"tr",align:null},"Aria device timestamp in microseconds")),(0,r.mdx)("tr",{parentName:"tbody"},(0,r.mdx)("td",{parentName:"tr",align:null},(0,r.mdx)("inlineCode",{parentName:"td"},"utc_timestamp_ns")),(0,r.mdx)("td",{parentName:"tr",align:null},"int"),(0,r.mdx)("td",{parentName:"tr",align:null},"Wall clock UTC time in nanoseconds. If not available, the value will be -1")),(0,r.mdx)("tr",{parentName:"tbody"},(0,r.mdx)("td",{parentName:"tr",align:null},(0,r.mdx)("inlineCode",{parentName:"td"},"session_uid")),(0,r.mdx)("td",{parentName:"tr",align:null},"string"),(0,r.mdx)("td",{parentName:"tr",align:null},"Unique identifier of the odometry coordinate frame. When the session_uid is the same, poses and velocities are defined in the same coordinate frame")),(0,r.mdx)("tr",{parentName:"tbody"},(0,r.mdx)("td",{parentName:"tr",align:null},(0,r.mdx)("inlineCode",{parentName:"td"},"{tx,ty,tz,qx,qy,qz,qw}_odometry_device")),(0,r.mdx)("td",{parentName:"tr",align:null},"float"),(0,r.mdx)("td",{parentName:"tr",align:null},"Pose of the device coordinate frame in odometry frame T_odometry_device, include translation (tx, ty, tz) in meters and rotation quaternion (qx, qy, qz, qw)")),(0,r.mdx)("tr",{parentName:"tbody"},(0,r.mdx)("td",{parentName:"tr",align:null},(0,r.mdx)("inlineCode",{parentName:"td"},"device_linear_velocity_{x,y,z}_odometry")),(0,r.mdx)("td",{parentName:"tr",align:null},"float"),(0,r.mdx)("td",{parentName:"tr",align:null},"Velocity of device coordinate frame in odometry frame, (x, y, z) in meter/s")),(0,r.mdx)("tr",{parentName:"tbody"},(0,r.mdx)("td",{parentName:"tr",align:null},(0,r.mdx)("inlineCode",{parentName:"td"},"angular_velocity_{x,y,z}_device")),(0,r.mdx)("td",{parentName:"tr",align:null},"float"),(0,r.mdx)("td",{parentName:"tr",align:null},"Angular velocity of device coordinate frame in device frame, (x, y, z) in rad/s")),(0,r.mdx)("tr",{parentName:"tbody"},(0,r.mdx)("td",{parentName:"tr",align:null},(0,r.mdx)("inlineCode",{parentName:"td"},"gravity_{x,y,z}_odometry")),(0,r.mdx)("td",{parentName:"tr",align:null},"float"),(0,r.mdx)("td",{parentName:"tr",align:null},"Earth gravity vector in odometry frame, (x, y, z) in meter/s^2. This vector is pointing toward the ground, and includes gravitation and centrifugal forces from earth rotation")),(0,r.mdx)("tr",{parentName:"tbody"},(0,r.mdx)("td",{parentName:"tr",align:null},(0,r.mdx)("inlineCode",{parentName:"td"},"quality_score")),(0,r.mdx)("td",{parentName:"tr",align:null},"float"),(0,r.mdx)("td",{parentName:"tr",align:null},"A quality score between 0.0 to 1.0. The larger the score is, the higher confidence the estimation has higher quality")))),(0,r.mdx)("h2",{id:"closed-loop-trajectory"},"Closed loop trajectory"),(0,r.mdx)("p",null,"Closed loop trajectory is the high frequency (IMU rate, which is 1kHz) pose estimation output by our mapping process, in an arbitrary gravity aligned world coordinate frame. The estimation includes pose and dynamics (translational and angular velocities)."),(0,r.mdx)("p",null,"Closed loop trajectories are fully bundle adjusted with detected loop closures, reducing the VIO drift which is present in the open loop trajectories. However, due to the loop closure correction, the \u201crelative\u201d and \u201clocal\u201d trajectory accuracy within a short time span (i.e. seconds) might be worse compared to open loop trajectories."),(0,r.mdx)("p",null,"In some open datasets we also share and use this format for trajectory pose ground truth from simulation or Optitrack, and the files will be called in a different file name aria_gt_trajectory.csv."),(0,r.mdx)("p",null,"For the utility function to load the closed loop trajectory in Python and C++, please check the ",(0,r.mdx)("a",{parentName:"p",href:"/docs/data_utilities/core_code_snippets/mps#open-loopclosed-loop-trajectory"},"code examples")),(0,r.mdx)("table",null,(0,r.mdx)("thead",{parentName:"table"},(0,r.mdx)("tr",{parentName:"thead"},(0,r.mdx)("th",{parentName:"tr",align:null},"Column"),(0,r.mdx)("th",{parentName:"tr",align:null},"Type"),(0,r.mdx)("th",{parentName:"tr",align:null},"Description"))),(0,r.mdx)("tbody",{parentName:"table"},(0,r.mdx)("tr",{parentName:"tbody"},(0,r.mdx)("td",{parentName:"tr",align:null},(0,r.mdx)("inlineCode",{parentName:"td"},"graph_uid")),(0,r.mdx)("td",{parentName:"tr",align:null},"string"),(0,r.mdx)("td",{parentName:"tr",align:null},"Unique identifier of the world coordinate frame")),(0,r.mdx)("tr",{parentName:"tbody"},(0,r.mdx)("td",{parentName:"tr",align:null},(0,r.mdx)("inlineCode",{parentName:"td"},"tracking_timestamp_us")),(0,r.mdx)("td",{parentName:"tr",align:null},"int"),(0,r.mdx)("td",{parentName:"tr",align:null},"Aria device timestamp in microsecond")),(0,r.mdx)("tr",{parentName:"tbody"},(0,r.mdx)("td",{parentName:"tr",align:null},(0,r.mdx)("inlineCode",{parentName:"td"},"utc_timestamp_ns")),(0,r.mdx)("td",{parentName:"tr",align:null},"int"),(0,r.mdx)("td",{parentName:"tr",align:null},"Wall clock UTC time in nanosecond. If not available, the value will be -1")),(0,r.mdx)("tr",{parentName:"tbody"},(0,r.mdx)("td",{parentName:"tr",align:null},(0,r.mdx)("inlineCode",{parentName:"td"},"{tx,ty,tz,qx,qy,qz,qw}_world_device")),(0,r.mdx)("td",{parentName:"tr",align:null},"float"),(0,r.mdx)("td",{parentName:"tr",align:null},"Pose of the device coordinate frame in world frame T_world_device, translation (tx, ty, tz) in meters and rotation quaternion (qx, qy, qz, qw)")),(0,r.mdx)("tr",{parentName:"tbody"},(0,r.mdx)("td",{parentName:"tr",align:null},(0,r.mdx)("inlineCode",{parentName:"td"},"device_linear_velocity_{x,y,z}_device")),(0,r.mdx)("td",{parentName:"tr",align:null},"float"),(0,r.mdx)("td",{parentName:"tr",align:null},"Velocity of device coordinate frame in device frame, (x, y, z) in meter/s")),(0,r.mdx)("tr",{parentName:"tbody"},(0,r.mdx)("td",{parentName:"tr",align:null},(0,r.mdx)("inlineCode",{parentName:"td"},"angular_velocity_{x,y,z}_device")),(0,r.mdx)("td",{parentName:"tr",align:null},"float"),(0,r.mdx)("td",{parentName:"tr",align:null},"Angular velocity of device coordinate frame in device frame, (x, y, z) in rad/s")),(0,r.mdx)("tr",{parentName:"tbody"},(0,r.mdx)("td",{parentName:"tr",align:null},(0,r.mdx)("inlineCode",{parentName:"td"},"gravity_{x,y,z}_world")),(0,r.mdx)("td",{parentName:"tr",align:null},"float"),(0,r.mdx)("td",{parentName:"tr",align:null},"Gravity vector (x, y, z) in the world frame, in meter/s^2. MPS output will all have fixed value` ","[0, 0, -9.81]","\u2019, while other source (e.g. simulation or Optitrack ground truth) may give different values")),(0,r.mdx)("tr",{parentName:"tbody"},(0,r.mdx)("td",{parentName:"tr",align:null},(0,r.mdx)("inlineCode",{parentName:"td"},"quality_score")),(0,r.mdx)("td",{parentName:"tr",align:null},"float"),(0,r.mdx)("td",{parentName:"tr",align:null},"A quality score between 0.0 to 1.0. The larger the score is, the higher confidence the estimation has higher quality`")))),(0,r.mdx)("h2",{id:"online-calibration"},"Online calibration"),(0,r.mdx)("p",null,"JSON files contain one json online calibration record per line. Each record is a json dict object that contains timestamp metadata and the result of online calibration for the cameras and IMUs. The calibration parameters contain ",(0,r.mdx)("a",{parentName:"p",href:"/docs/tech_insights/camera_intrinsic_models"},"intrinsics"),") and ",(0,r.mdx)("a",{parentName:"p",href:"/docs/data_formats/coordinate_convention/3d_coordinate_frame_convention"},"extrinsics")," parameters for each sensor as well as a time offsets which best temporally align their data. For how to load and read online calibrations in Python and C++, please checkout the ",(0,r.mdx)("a",{parentName:"p",href:"/docs/data_utilities/core_code_snippets/mps#online-calibration"},"code examples")),(0,r.mdx)("h2",{id:"static-camera-calibration"},"Static camera calibration"),(0,r.mdx)("p",null,"Poses and intrinsic calibration of a set of stationary cameras. For the utility function to load the static cameras in Python and C++, please check the ",(0,r.mdx)("a",{parentName:"p",href:"/docs/data_utilities/core_code_snippets/mps#static-camera-calibration"},"code examples")),(0,r.mdx)("table",null,(0,r.mdx)("thead",{parentName:"table"},(0,r.mdx)("tr",{parentName:"thead"},(0,r.mdx)("th",{parentName:"tr",align:null},"Column"),(0,r.mdx)("th",{parentName:"tr",align:null},"Type"),(0,r.mdx)("th",{parentName:"tr",align:null},"Description"))),(0,r.mdx)("tbody",{parentName:"table"},(0,r.mdx)("tr",{parentName:"tbody"},(0,r.mdx)("td",{parentName:"tr",align:null},(0,r.mdx)("inlineCode",{parentName:"td"},"cam_uid")),(0,r.mdx)("td",{parentName:"tr",align:null},"string"),(0,r.mdx)("td",{parentName:"tr",align:null},"Unique identifier of camera")),(0,r.mdx)("tr",{parentName:"tbody"},(0,r.mdx)("td",{parentName:"tr",align:null},(0,r.mdx)("inlineCode",{parentName:"td"},"graph_uid")),(0,r.mdx)("td",{parentName:"tr",align:null},"string"),(0,r.mdx)("td",{parentName:"tr",align:null},"Unique identifier of the world coordinate frame")),(0,r.mdx)("tr",{parentName:"tbody"},(0,r.mdx)("td",{parentName:"tr",align:null},(0,r.mdx)("inlineCode",{parentName:"td"},"{tx,ty,tz,qx,qy,qz,qw}_world_cam")),(0,r.mdx)("td",{parentName:"tr",align:null},"float"),(0,r.mdx)("td",{parentName:"tr",align:null},"Pose of the camera coordinate frame in world frame T_world_cam, translation (tx, ty, tz) in meters and rotation quaternion (qx, qy, qz, qw)")),(0,r.mdx)("tr",{parentName:"tbody"},(0,r.mdx)("td",{parentName:"tr",align:null},(0,r.mdx)("inlineCode",{parentName:"td"},"image_width")),(0,r.mdx)("td",{parentName:"tr",align:null},"int"),(0,r.mdx)("td",{parentName:"tr",align:null},"Image size in pixels")),(0,r.mdx)("tr",{parentName:"tbody"},(0,r.mdx)("td",{parentName:"tr",align:null},(0,r.mdx)("inlineCode",{parentName:"td"},"image_height")),(0,r.mdx)("td",{parentName:"tr",align:null},"int"),(0,r.mdx)("td",{parentName:"tr",align:null},"Image size in pixels")),(0,r.mdx)("tr",{parentName:"tbody"},(0,r.mdx)("td",{parentName:"tr",align:null},(0,r.mdx)("inlineCode",{parentName:"td"},"intrinsics_type")),(0,r.mdx)("td",{parentName:"tr",align:null},"string"),(0,r.mdx)("td",{parentName:"tr",align:null},"Camera intrinsics calibration type. Currently support types: ",(0,r.mdx)("inlineCode",{parentName:"td"},"KANNALABRANDTK3"),": ",(0,r.mdx)("a",{parentName:"td",href:"/docs/tech_insights/camera_intrinsic_models#the-kannalabrandtk3-kb3-model"},"KB3 model"))),(0,r.mdx)("tr",{parentName:"tbody"},(0,r.mdx)("td",{parentName:"tr",align:null},(0,r.mdx)("inlineCode",{parentName:"td"},"intrinsics_{0-7}")),(0,r.mdx)("td",{parentName:"tr",align:null},"float"),(0,r.mdx)("td",{parentName:"tr",align:null},"Camera intrinsics parameters")),(0,r.mdx)("tr",{parentName:"tbody"},(0,r.mdx)("td",{parentName:"tr",align:null},(0,r.mdx)("inlineCode",{parentName:"td"},"start_frame_idx")),(0,r.mdx)("td",{parentName:"tr",align:null},"int"),(0,r.mdx)("td",{parentName:"tr",align:null},"The start frame number of the video is stationary, and camera pose and intrinsic calibration results are applicable. Both will be -1 if the pose and intrinsic calibration are applicable to the whole video")),(0,r.mdx)("tr",{parentName:"tbody"},(0,r.mdx)("td",{parentName:"tr",align:null},(0,r.mdx)("inlineCode",{parentName:"td"},"end_frame_idx")),(0,r.mdx)("td",{parentName:"tr",align:null},"int"),(0,r.mdx)("td",{parentName:"tr",align:null},"The end frame number of the video is stationary, and camera pose and intrinsic calibration results are applicable. Both will be -1 if the pose and intrinsic calibration are applicable to the whole video")))))}s.isMDXComponent=!0}}]);