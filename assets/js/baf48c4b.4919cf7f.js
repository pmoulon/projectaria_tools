"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[3199],{15680:(e,t,a)=>{a.r(t),a.d(t,{MDXContext:()=>c,MDXProvider:()=>m,mdx:()=>y,useMDXComponents:()=>l,withMDXComponents:()=>d});var r=a(96540);function o(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function i(){return i=Object.assign||function(e){for(var t=1;t<arguments.length;t++){var a=arguments[t];for(var r in a)Object.prototype.hasOwnProperty.call(a,r)&&(e[r]=a[r])}return e},i.apply(this,arguments)}function n(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,r)}return a}function s(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?n(Object(a),!0).forEach((function(t){o(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):n(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function p(e,t){if(null==e)return{};var a,r,o=function(e,t){if(null==e)return{};var a,r,o={},i=Object.keys(e);for(r=0;r<i.length;r++)a=i[r],t.indexOf(a)>=0||(o[a]=e[a]);return o}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(r=0;r<i.length;r++)a=i[r],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(o[a]=e[a])}return o}var c=r.createContext({}),d=function(e){return function(t){var a=l(t.components);return r.createElement(e,i({},t,{components:a}))}},l=function(e){var t=r.useContext(c),a=t;return e&&(a="function"==typeof e?e(t):s(s({},t),e)),a},m=function(e){var t=l(e.components);return r.createElement(c.Provider,{value:t},e.children)},u="mdxType",f={inlineCode:"code",wrapper:function(e){var t=e.children;return r.createElement(r.Fragment,{},t)}},h=r.forwardRef((function(e,t){var a=e.components,o=e.mdxType,i=e.originalType,n=e.parentName,c=p(e,["components","mdxType","originalType","parentName"]),d=l(a),m=o,u=d["".concat(n,".").concat(m)]||d[m]||f[m]||i;return a?r.createElement(u,s(s({ref:t},c),{},{components:a})):r.createElement(u,s({ref:t},c))}));function y(e,t){var a=arguments,o=t&&t.mdxType;if("string"==typeof e||o){var i=a.length,n=new Array(i);n[0]=h;var s={};for(var p in t)hasOwnProperty.call(t,p)&&(s[p]=t[p]);s.originalType=e,s[u]="string"==typeof e?e:o,n[1]=s;for(var c=2;c<i;c++)n[c]=a[c];return r.createElement.apply(null,n)}return r.createElement.apply(null,a)}h.displayName="MDXCreateElement"},41699:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>p,contentTitle:()=>n,default:()=>m,frontMatter:()=>i,metadata:()=>s,toc:()=>c});var r=a(58168),o=(a(96540),a(15680));const i={sidebar_position:10,title:"Machine Perception Services (MPS)"},n="Project Aria Machine Perception Services",s={unversionedId:"ARK/mps/mps",id:"ARK/mps/mps",title:"Machine Perception Services (MPS)",description:"To accelerate research with Project Aria, we provide several Spatial AI machine perception capabilities that form the foundation for the future Contextualized AI applications and analysis of the egocentric data. These capabilities are powered by a set of proprietary machine perception algorithms designed for Project Aria glasses and provide superior accuracy and robustness on recorded data compared to off-the-shelf open source algorithms.",source:"@site/docs/ARK/mps/mps.mdx",sourceDirName:"ARK/mps",slug:"/ARK/mps/",permalink:"/projectaria_tools/docs/ARK/mps/",draft:!1,editUrl:"https://github.com/facebookresearch/projectaria_tools/tree/main/website/docs/ARK/mps/mps.mdx",tags:[],version:"current",sidebarPosition:10,frontMatter:{sidebar_position:10,title:"Machine Perception Services (MPS)"},sidebar:"tutorialSidebar",previous:{title:"Desktop Companion App",permalink:"/projectaria_tools/docs/ARK/desktop_companion_app"},next:{title:"Request MPS",permalink:"/projectaria_tools/docs/ARK/mps/request_mps"}},p={},c=[{value:"Current MPS offerings",id:"current-mps-offerings",level:2},{value:"6DoF trajectory",id:"6dof-trajectory",level:3},{value:"Online sensor calibration",id:"online-sensor-calibration",level:3},{value:"Semi-dense point cloud",id:"semi-dense-point-cloud",level:3},{value:"Eye gaze data",id:"eye-gaze-data",level:3},{value:"About MPS Data Loader APIs",id:"about-mps-data-loader-apis",level:2},{value:"Questions &amp; Feedback",id:"questions--feedback",level:2}],d={toc:c},l="wrapper";function m(e){let{components:t,...a}=e;return(0,o.mdx)(l,(0,r.A)({},d,a,{components:t,mdxType:"MDXLayout"}),(0,o.mdx)("h1",{id:"project-aria-machine-perception-services"},"Project Aria Machine Perception Services"),(0,o.mdx)("p",null,"To accelerate research with Project Aria, we provide several Spatial AI machine perception capabilities that form the foundation for the future Contextualized AI applications and analysis of the egocentric data. These capabilities are powered by a set of proprietary machine perception algorithms designed for Project Aria glasses and provide superior accuracy and robustness on recorded data compared to off-the-shelf open source algorithms."),(0,o.mdx)("p",null,"All MPS functionalities are offered as post-processing of VRS files via a cloud service. Use the ",(0,o.mdx)("a",{parentName:"p",href:"/docs/ARK/desktop_companion_app"},"Desktop App"),", to ",(0,o.mdx)("a",{parentName:"p",href:"/docs/ARK/mps/request_mps"},"request")," derived data from any VRS file that contains necessary sensor's data."),(0,o.mdx)("h2",{id:"current-mps-offerings"},"Current MPS offerings"),(0,o.mdx)("p",null,"The following MPS can be requested via the Aria Desktop app for data you've recorded with a Project Aria device, as long as the data has been recorded with a compatible Recording Profile. The ",(0,o.mdx)("a",{parentName:"p",href:"/docs/ARK/glasses_manual/profile_guide"},"Recording Profile Guide")," provides a quick list of compatible sensor profiles or go to ",(0,o.mdx)("a",{parentName:"p",href:"/docs/tech_spec/recording_profiles"},"Recording Profiles")," in Technical Specifications for more granular information about each profile."),(0,o.mdx)("h3",{id:"6dof-trajectory"},"6DoF trajectory"),(0,o.mdx)("p",null,"MPS provides two types of high frequency (1kHz) trajectories"),(0,o.mdx)("ul",null,(0,o.mdx)("li",{parentName:"ul"},(0,o.mdx)("a",{parentName:"li",href:"/docs/data_formats/mps/mps_trajectory#open-loop-trajectory"},"Open loop trajectory")," that is a local odometry estimation from visual-inertial odometry (VIO)"),(0,o.mdx)("li",{parentName:"ul"},(0,o.mdx)("a",{parentName:"li",href:"/docs/data_formats/mps/mps_trajectory#closed-loop-trajectory"},"Closed loop trajectory")," that is created via batch optimization, using multi-sensors' input (SLAM, IMU, barometer, Wi-Fi and GPS), fully optimized and providing poses in a consistent frame of reference.")),(0,o.mdx)("p",null,"Request trajectory (location data) in the Desktop app to get these outputs, the ",(0,o.mdx)("a",{parentName:"p",href:"/docs/tech_spec/recording_profiles"},"recording profile")," must have SLAM cameras + IMU enabled."),(0,o.mdx)("h3",{id:"online-sensor-calibration"},"Online sensor calibration"),(0,o.mdx)("p",null,"The ",(0,o.mdx)("a",{parentName:"p",href:"/docs/data_formats/mps/mps_trajectory#online-calibration"},"time-varying intrinsic and extrinsic calibrations")," of cameras and IMUs are estimated at the frequency of SLAM cameras by our multi-sensor state estimation pipeline."),(0,o.mdx)("p",null,"Request trajectory (location data) in the Desktop app to get these outputs, the ",(0,o.mdx)("a",{parentName:"p",href:"/docs/tech_spec/recording_profiles"},"recording profile")," must have SLAM cameras + IMU enabled."),(0,o.mdx)("h3",{id:"semi-dense-point-cloud"},"Semi-dense point cloud"),(0,o.mdx)("p",null,(0,o.mdx)("a",{parentName:"p",href:"/docs/data_formats/mps/mps_pointcloud"},"Semi-dense point cloud")," data supports researchers who need static scene 3D reconstructions, reliable 2D images tracks or a representative visualization of the environment."),(0,o.mdx)("p",null,"In the Desktop app, this can be requested as an addition to trajectory (location) derived data and has the same recording profile requirements."),(0,o.mdx)("h3",{id:"eye-gaze-data"},"Eye gaze data"),(0,o.mdx)("p",null,"Eye gaze is the most important indicator of human\u2019s attention, ",(0,o.mdx)("a",{parentName:"p",href:"/docs/data_formats/mps/mps_eye_gaze"},"eye gaze direction")," estimation with uncertainty is provided by MPS. Eye gaze estimation uses the data from the Eye Tracking (ET) cameras."),(0,o.mdx)("p",null,"Request Eye Gaze data in the Desktop app to get these outputs, for any recording that had ",(0,o.mdx)("a",{parentName:"p",href:"/docs/tech_spec/recording_profiles"},"ET cameras enabled"),"."),(0,o.mdx)("p",null,"If you have made a recording with ",(0,o.mdx)("a",{parentName:"p",href:"/docs/ARK/mps/eye_gaze_calibration"},"In-Session Eye Gaze Calibration"),", you will receive a second .csv file with calibrated eye gaze outputs."),(0,o.mdx)("h2",{id:"about-mps-data-loader-apis"},"About MPS Data Loader APIs"),(0,o.mdx)("p",null,"Please refer to our ",(0,o.mdx)("a",{parentName:"p",href:"/docs/data_utilities/core_code_snippets/mps#load-mps-output"},"MPS data loader APIs")," (C++ and Python support) to load the MPS outputs into your application. Additionally, the ",(0,o.mdx)("a",{parentName:"p",href:"/docs/data_utilities/visualization/visualization_cpp#mps-static-scene-visualizer"},"visualization guide")," shows how to run our rich visualization tools to visualize all the MPS outputs."),(0,o.mdx)("h2",{id:"questions--feedback"},"Questions & Feedback"),(0,o.mdx)("p",null,"If you have feedback you'd like to provide about overall trends and experiences or improvement ideas we'd love to hear from you. Please email ",(0,o.mdx)("a",{parentName:"p",href:"mailto:AriaOps@meta.com"},"AriaOps@meta.com")," or post to the Project Aria Academic Partner Feedback and Support group."))}m.isMDXComponent=!0}}]);