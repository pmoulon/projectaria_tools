"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[4283],{15680:(e,t,o)=>{o.r(t),o.d(t,{MDXContext:()=>c,MDXProvider:()=>m,mdx:()=>x,useMDXComponents:()=>p,withMDXComponents:()=>d});var a=o(96540);function r(e,t,o){return t in e?Object.defineProperty(e,t,{value:o,enumerable:!0,configurable:!0,writable:!0}):e[t]=o,e}function n(){return n=Object.assign||function(e){for(var t=1;t<arguments.length;t++){var o=arguments[t];for(var a in o)Object.prototype.hasOwnProperty.call(o,a)&&(e[a]=o[a])}return e},n.apply(this,arguments)}function i(e,t){var o=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),o.push.apply(o,a)}return o}function s(e){for(var t=1;t<arguments.length;t++){var o=null!=arguments[t]?arguments[t]:{};t%2?i(Object(o),!0).forEach((function(t){r(e,t,o[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(o)):i(Object(o)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(o,t))}))}return e}function l(e,t){if(null==e)return{};var o,a,r=function(e,t){if(null==e)return{};var o,a,r={},n=Object.keys(e);for(a=0;a<n.length;a++)o=n[a],t.indexOf(o)>=0||(r[o]=e[o]);return r}(e,t);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);for(a=0;a<n.length;a++)o=n[a],t.indexOf(o)>=0||Object.prototype.propertyIsEnumerable.call(e,o)&&(r[o]=e[o])}return r}var c=a.createContext({}),d=function(e){return function(t){var o=p(t.components);return a.createElement(e,n({},t,{components:o}))}},p=function(e){var t=a.useContext(c),o=t;return e&&(o="function"==typeof e?e(t):s(s({},t),e)),o},m=function(e){var t=p(e.components);return a.createElement(c.Provider,{value:t},e.children)},u="mdxType",g={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},f=a.forwardRef((function(e,t){var o=e.components,r=e.mdxType,n=e.originalType,i=e.parentName,c=l(e,["components","mdxType","originalType","parentName"]),d=p(o),m=r,u=d["".concat(i,".").concat(m)]||d[m]||g[m]||n;return o?a.createElement(u,s(s({ref:t},c),{},{components:o})):a.createElement(u,s({ref:t},c))}));function x(e,t){var o=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var n=o.length,i=new Array(n);i[0]=f;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s[u]="string"==typeof e?e:r,i[1]=s;for(var c=2;c<n;c++)i[c]=o[c];return a.createElement.apply(null,i)}return a.createElement.apply(null,o)}f.displayName="MDXCreateElement"},86319:(e,t,o)=>{o.r(t),o.d(t,{assets:()=>l,contentTitle:()=>i,default:()=>m,frontMatter:()=>n,metadata:()=>s,toc:()=>c});var a=o(58168),r=(o(96540),o(15680));const n={sidebar_position:10,title:"Ego-Exo4D Dataset"},i=void 0,s={unversionedId:"open_datasets/ego-exo4d/ego-exo4d",id:"open_datasets/ego-exo4d/ego-exo4d",title:"Ego-Exo4D Dataset",description:"Ego-Exo4D is a foundational dataset for research on video learning and multimodal perception. The open dataset was released in November 2023 and in March 2024 was updated to include more data, more annotations, and more modalities. Ego-Exo4D comes from years of collaboration between Meta\u2019s FAIR (Fundamental Artificial Intelligence Research), Meta\u2019s Project Aria, and 15 university partners. Ego-Exo4D is unique because of its simultaneous capture of:",source:"@site/docs/open_datasets/ego-exo4d/ego-exo4d.mdx",sourceDirName:"open_datasets/ego-exo4d",slug:"/open_datasets/ego-exo4d/",permalink:"/projectaria_tools/docs/open_datasets/ego-exo4d/",draft:!1,editUrl:"https://github.com/facebookresearch/projectaria_tools/tree/main/website/docs/open_datasets/ego-exo4d/ego-exo4d.mdx",tags:[],version:"current",sidebarPosition:10,frontMatter:{sidebar_position:10,title:"Ego-Exo4D Dataset"},sidebar:"tutorialSidebar",previous:{title:"ASE Challenges",permalink:"/projectaria_tools/docs/open_datasets/aria_synthetic_environments_dataset/ase_challenges"},next:{title:"Ego-Exo4D Data Format and Loader",permalink:"/projectaria_tools/docs/open_datasets/ego-exo4d/ego-exo4d_data_format"}},l={},c=[{value:"Resources:",id:"resources",level:3},{value:"Ego-Exo4D resources on Project Aria Tools",id:"ego-exo4d-resources-on-project-aria-tools",level:2}],d={toc:c},p="wrapper";function m(e){let{components:t,...o}=e;return(0,r.mdx)(p,(0,a.A)({},d,o,{components:t,mdxType:"MDXLayout"}),(0,r.mdx)("p",null,"Ego-Exo4D is a foundational dataset for research on video learning and multimodal perception. The open dataset ",(0,r.mdx)("a",{parentName:"p",href:"https://ai.meta.com/blog/ego-exo4d-video-learning-perception/"},"was released in November 2023")," and in March 2024 was updated to include more data, more annotations, and more modalities. Ego-Exo4D comes from years of collaboration between Meta\u2019s ",(0,r.mdx)("a",{parentName:"p",href:"https://ai.meta.com/research/"},"FAIR (Fundamental Artificial Intelligence Research)"),", Meta\u2019s ",(0,r.mdx)("a",{parentName:"p",href:"https://www.projectaria.com/"},"Project Aria"),", and 15 university partners. Ego-Exo4D is unique because of its simultaneous capture of:"),(0,r.mdx)("ul",null,(0,r.mdx)("li",{parentName:"ul"},"First-person \u201cegocentric\u201d views, from a participant\u2019s wearable camera"),(0,r.mdx)("li",{parentName:"ul"},"Multiple \u201cexocentric\u201d views, from cameras surrounding the participant")),(0,r.mdx)("p",null,"The two perspectives are complementary. While the egocentric perspective reveals what the participant sees and hears, the exocentric views reveal the surrounding scene and the context. Together, these two perspectives give AI models a new window into complex human skill."),(0,r.mdx)("p",null,"More than 800 participants from 13 cities worldwide performed these activities in 131 different natural scene contexts, yielding long-form captures from 1 to 42 minutes each and 1,422 hours of video combined. The dataset contains skilled activities covering both physical (Soccer, Basketball, Dance, Bouldering, Music) and procedural (Cooking, Bike Repair, Health) tasks."),(0,r.mdx)("p",null,"Ego-Exo4D has its own independent website and documentation. Project Aria Tools provides documentation and tooling for working with Aria data that can be helpful when working with Ego-Exo4D data."),(0,r.mdx)("h3",{id:"resources"},"Resources:"),(0,r.mdx)("ul",null,(0,r.mdx)("li",{parentName:"ul"},(0,r.mdx)("a",{parentName:"li",href:"https://ego-exo4d-data.org/"},"Ego-Exo4D website")),(0,r.mdx)("li",{parentName:"ul"},(0,r.mdx)("a",{parentName:"li",href:"https://docs.ego-exo4d-data.org/"},"Ego-Exo4D documentation")),(0,r.mdx)("li",{parentName:"ul"},(0,r.mdx)("a",{parentName:"li",href:"https://github.com/facebookresearch/Ego4d/blob/main/notebooks/egoexo/EgoExo_Aria_Data_Tutorial.ipynb"},"EgoExo4D Tutorial")),(0,r.mdx)("li",{parentName:"ul"},(0,r.mdx)("a",{parentName:"li",href:"https://discuss.ego4d-data.org/"},"Ego4D and Ego-Exo4D feedback and support"))),(0,r.mdx)("h2",{id:"ego-exo4d-resources-on-project-aria-tools"},"Ego-Exo4D resources on Project Aria Tools"),(0,r.mdx)("p",null,"There are a range of resources available for working with Aria data using Project Aria Tools, and Load Static Calibration Data was specifically created to support working with Ego-Exo4D data."),(0,r.mdx)("ul",null,(0,r.mdx)("li",{parentName:"ul"},(0,r.mdx)("a",{parentName:"li",href:"/projectaria_tools/docs/open_datasets/ego-exo4d/ego-exo4d_data_format"},"EgoExo4D Data Format and Loader")),(0,r.mdx)("li",{parentName:"ul"},(0,r.mdx)("a",{parentName:"li",href:"/projectaria_tools/docs/data_utilities/getting_started"},"Getting Started With Project Aria Data Utilities")),(0,r.mdx)("li",{parentName:"ul"},(0,r.mdx)("a",{parentName:"li",href:"/projectaria_tools/docs/data_formats/"},"Aria Data Formats"))))}m.isMDXComponent=!0}}]);