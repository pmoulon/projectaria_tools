{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AEA Tutorial\n",
    "In this tutorial we will walk through the steps to access [Aria Everyday Activities (AEA) dataset](https://www.projectaria.com/datasets/aea/) The AEA dataset contains:\n",
    "* Raw data from 1-2 [Project Aria glasses](https://projectaria.com)\n",
    "    * Data is synchronized where recordings are made with two Aria glasses in the same location\n",
    "* Speech to Text annotation\n",
    "* Machine Perception Services [(MPS)](https://facebookresearch.github.io/projectaria_tools/docs/ARK/mps) derived data:\n",
    "    * General Eye Gaze \n",
    "    * Trajectory \n",
    "        * Open loop trajectories\n",
    "        * Closed loop trajectories\n",
    "        * Calibration data\n",
    "    * 3D Point Clouds\n",
    "  \n",
    "This tutorial covers how to:\n",
    "* Access raw sensor data (VRS files)\n",
    "* Visualize Eye Gaze data\n",
    "* Visualize Speech data\n",
    "* Load concurrent sequences from multiple Project Aria glasses in a shared space location\n",
    "* Use Timecode to get synchronized timestamps \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifics for Google Colab\n",
    "google_colab_env = 'google.colab' in str(get_ipython())\n",
    "if google_colab_env:\n",
    "    print(\"Running from Google Colab, installing projectaria_tools and getting sample data\")\n",
    "    !pip install projectaria-tools>=1.3.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from projectaria_tools.projects.aea import (\n",
    "    AriaEverydayActivitiesDataPathsProvider, \n",
    "    AriaEverydayActivitiesDataProvider)\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from projectaria_tools.core import calibration, mps\n",
    "from projectaria_tools.core.sensor_data import TimeDomain, TimeQueryOptions\n",
    "from projectaria_tools.core.mps import get_eyegaze_point_at_depth\n",
    "\n",
    "from projectaria_tools.core.sophus import SE3\n",
    "from projectaria_tools.core.stream_id import StreamId"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create AEA data provider\n",
    "The AEA data provider is similar to the [Project Aria Tools data provider](https://facebookresearch.github.io/projectaria_tools/docs/data_utilities/core_code_snippets/data_provider), with the additional ability to access speech data and data from multiple concurrent recordings. \n",
    "\n",
    "* Create AEA data provider and access all the following providers\n",
    "    * mps provider\n",
    "    * speech provider\n",
    "    * vrs provider\n",
    "* Create AEA data_path_provider and access the specific file directory\n",
    "    * Get sequence meta data\n",
    "    * Get paths of vrs, mps, and speech\n",
    "    * Get path of concurrent recording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if google_colab_env:\n",
    "    aea_sample_path = \"./aea_sample_data\"\n",
    "else:\n",
    "    aea_sample_path = \"/tmp/aea_sample_data\"\n",
    "\n",
    "data_sequence_url_rec1 = \"https://www.projectaria.com/async/sample/download/?bucket=aea&filename=loc1_script2_seq1_rec1_10s_sample.zip\"\n",
    "data_sequence_url_rec2 = \"https://www.projectaria.com/async/sample/download/?bucket=aea&filename=loc1_script2_seq1_rec2_10s_sample.zip\"\n",
    "\n",
    "sequence_name_rec1 = \"loc1_script2_seq1_rec1_10s_sample\"\n",
    "sequence_name_rec2 = \"loc1_script2_seq1_rec2_10s_sample\"\n",
    "\n",
    "sequence_path_rec1 = f\"{aea_sample_path}/{sequence_name_rec1}\"\n",
    "sequence_path_rec2 = f\"{aea_sample_path}/{sequence_name_rec2}\"\n",
    "\n",
    "command_list = [\n",
    "    f\"mkdir -p {aea_sample_path}\",\n",
    "    f\"mkdir -p {sequence_path_rec1}\",\n",
    "    f\"mkdir -p {sequence_path_rec2}\",\n",
    "    # Download sample data\n",
    "    f'curl -o {aea_sample_path}/aea_sample_data_rec1.zip -C - -O -L \"{data_sequence_url_rec1}\"',\n",
    "    f'curl -o {aea_sample_path}/aea_sample_data_rec2.zip -C - -O -L \"{data_sequence_url_rec2}\"',\n",
    "    # Unzip the sample data\n",
    "    f\"unzip -o {aea_sample_path}/aea_sample_data_rec1.zip -d {sequence_path_rec1}\",\n",
    "    f\"unzip -o {aea_sample_path}/aea_sample_data_rec2.zip -d {sequence_path_rec2}\",\n",
    "\n",
    "]\n",
    "\n",
    "# Execute the commands for downloading dataset\n",
    "if google_colab_env:\n",
    "    for command in command_list:\n",
    "        !$command\n",
    "else:\n",
    "    for command in command_list:\n",
    "        subprocess.run(command, shell=True, check=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create AEA path provider\n",
    "aea_paths_provider_1 = AriaEverydayActivitiesDataPathsProvider(sequence_path_rec1)\n",
    "\n",
    "# create AEA data provider\n",
    "aea_data_provider_1 = AriaEverydayActivitiesDataProvider(sequence_path_rec1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show AEA data paths\n",
    "AEA data path from a sequence can be obtained using AEA path provider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(aea_paths_provider_1.get_data_paths())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access concurrent recordings\n",
    "Query metadata.json to find and load the directory of any concurrent recording into the AEA data_path_provider.\n",
    "If your recording doesn’t have a concurrent recording you’ll get the error: “RuntimeError: Timedomain TimeCode not supported by stream RGB Camera Class #1”.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get concurrent recording filename and load into AEA data provider\n",
    "concurrent_recording_filename = aea_paths_provider_1.get_concurrent_recordings()\n",
    "concurrent_sequence_path = os.path.join(os.path.dirname(sequence_path_rec1), concurrent_recording_filename[0])\n",
    "\n",
    "# Load concurrent sequence directly\n",
    "aea_data_provider_2 = AriaEverydayActivitiesDataProvider(concurrent_sequence_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access synchronized timestamp from both provider\n",
    "Timestamp can come from four different places:\n",
    "* TimeDomain.DEVICE_TIME\n",
    "* TimeDomain.TIME_CODE\n",
    "* TimeDomain.RECORD_TIME\n",
    "* TimeDomain.HOST_TIME\n",
    "\n",
    "**TimeDomain.TIME_CODE** is commonly used to register timestamp of multiple synchronized recordings.\n",
    "\n",
    "For AEA dataset, TimeDomain.TIME_CODE is used first to find match and TimeDomain.DeviceTime is used to query data from MPS and speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RGB_STREAM_ID = StreamId(\"214-1\")\n",
    "\n",
    "# timecode timestamps for recording 1\n",
    "timecode_vec_1 = aea_data_provider_1.vrs.get_timestamps_ns(RGB_STREAM_ID, TimeDomain.TIME_CODE)\n",
    "\n",
    "# timecode timestamps for recording 2\n",
    "timecode_vec_2 = aea_data_provider_2.vrs.get_timestamps_ns(RGB_STREAM_ID, TimeDomain.TIME_CODE)\n",
    "\n",
    "# Convert the common timecode timestamp to unique DeviceTime timestamp for each sequence\n",
    "for timecode_time_ns in timecode_vec_1:\n",
    "        device_time_ns_1 = aea_data_provider_1.vrs.convert_from_timecode_to_device_time_ns(timecode_time_ns)\n",
    "        device_time_ns_2 = aea_data_provider_2.vrs.convert_from_timecode_to_device_time_ns(timecode_time_ns)\n",
    "        print(f\"TimeCode {timecode_time_ns} converts to DeviceTime of sequence 1: {device_time_ns_1} and sequence 2: {device_time_ns_2}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access speech data\n",
    "Obtain speech data provider directly from aea_data_provider and query the speech data by sentence or words with timestamp in nanoseconds.\n",
    "* Speech data can be only be queried with `TimeDomain.DEVICE_TIME`\n",
    "* Timestamp conversion from / to `TimeDomain.TIME_CODE` using `convert_from_timecode_to_device_time_ns`\n",
    "* Obtain sentence/words from speech data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timecode_timestamp = timecode_vec_1[100]\n",
    "device_time_ns_1 = aea_data_provider_1.vrs.convert_from_timecode_to_device_time_ns(timecode_time_ns)\n",
    "device_time_ns_2 = aea_data_provider_2.vrs.convert_from_timecode_to_device_time_ns(timecode_time_ns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain sentence at query_time_ns\n",
    "sentence = aea_data_provider_1.speech.get_sentence_data_by_timestamp_ns(device_time_ns_1, TimeQueryOptions.CLOSEST)\n",
    "print(f\"Sentence from sequence 1 at device timestamp {device_time_ns_1} is : '{sentence}'\")\n",
    "\n",
    "# obtain word at query_time_ns\n",
    "sentence = aea_data_provider_2.speech.get_sentence_data_by_timestamp_ns(device_time_ns_2, TimeQueryOptions.CLOSEST)\n",
    "print(f\"Sentence from sequence 2 at device timestamp {device_time_ns_2} is : '{sentence}'\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize RGB image with Gaze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_eye_gaze(aea_data_provider : AriaEverydayActivitiesDataProvider, device_time_ns: int, depth_m: float):\n",
    "    rgb_stream_label = aea_data_provider.vrs.get_label_from_stream_id(RGB_STREAM_ID)\n",
    "    device_calibration = aea_data_provider.vrs.get_device_calibration()\n",
    "    rgb_camera_calibration = device_calibration.get_camera_calib(rgb_stream_label)\n",
    "    \n",
    "    image = aea_data_provider.vrs.get_image_data_by_time_ns(\n",
    "            RGB_STREAM_ID, device_time_ns, TimeDomain.DEVICE_TIME, TimeQueryOptions.BEFORE)    \n",
    "    eye_gaze = aea_data_provider.mps.get_general_eyegaze(device_time_ns, TimeQueryOptions.CLOSEST)\n",
    "    \n",
    "    assert eye_gaze, \"Eye gaze not available\"\n",
    "    # Compute eye_gaze vector at depth_m reprojection in the image\n",
    "    gaze_vector_in_cpf = mps.get_eyegaze_point_at_depth(\n",
    "        eye_gaze.yaw, eye_gaze.pitch, depth_m\n",
    "    )\n",
    "    T_device_CPF = device_calibration.get_transform_device_cpf()\n",
    "    gaze_center_in_camera = (\n",
    "        rgb_camera_calibration.get_transform_device_camera().inverse()\n",
    "        @ T_device_CPF\n",
    "        @ gaze_vector_in_cpf\n",
    "    )\n",
    "    gaze_projection = rgb_camera_calibration.project(gaze_center_in_camera)\n",
    "    \n",
    "    # Project the gaze center in rgb camera frame\n",
    "    if gaze_projection is not None:\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.imshow(image[0].to_numpy_array());\n",
    "        ax.axis('off')\n",
    "        \n",
    "        # Plot the cross\n",
    "        u, v = gaze_projection.flatten()\n",
    "        plt.plot(u, v, '+', c=\"red\", mew=1, ms=20); plt.xticks([]); plt.yticks([]); \n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"Eye gaze center projected to {gaze_projection}, which is out of camera sensor plane.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw general eye gaze with synchronized device timestamp at depth = 1.0 meter\n",
    "eye_gaze_depth = 1.0 # meters\n",
    "draw_eye_gaze(aea_data_provider_1, device_time_ns_1, eye_gaze_depth)\n",
    "draw_eye_gaze(aea_data_provider_2, device_time_ns_2, eye_gaze_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "custom": {
   "cells": [],
   "metadata": {
    "custom": {
     "cells": [],
     "metadata": {
      "custom": {
       "cells": [],
       "metadata": {
        "fileHeader": "",
        "fileUid": "07f6fe44-5558-4c96-b69d-656c9e2fc4c2",
        "isAdHoc": false,
        "kernelspec": {
         "display_name": "Python 3 (ipykernel)",
         "language": "python",
         "name": "python3"
        },
        "language_info": {
         "codemirror_mode": {
          "name": "ipython",
          "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.11.5"
        }
       },
       "nbformat": 4,
       "nbformat_minor": 5
      },
      "fileHeader": "",
      "fileUid": "262809d5-bb0d-4d4c-a2ed-b85174a862f0",
      "indentAmount": 2,
      "isAdHoc": false,
      "kernelspec": {
       "display_name": "Python 3 (ipykernel)",
       "language": "python",
       "name": "python3"
      },
      "language_info": {
       "codemirror_mode": {
        "name": "ipython",
        "version": 3
       },
       "file_extension": ".py",
       "mimetype": "text/x-python",
       "name": "python",
       "nbconvert_exporter": "python",
       "pygments_lexer": "ipython3",
       "version": "3.11.5"
      }
     },
     "nbformat": 4,
     "nbformat_minor": 4
    },
    "fileHeader": "",
    "fileUid": "de0f180a-2fa5-4edf-8c65-a031c3f55d76",
    "indentAmount": 2,
    "isAdHoc": false,
    "language_info": {
     "name": "plaintext"
    }
   },
   "nbformat": 4,
   "nbformat_minor": 2
  },
  "indentAmount": 2,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
